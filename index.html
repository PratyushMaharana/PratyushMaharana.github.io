<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Pratyush | AI & Bio Portfolio</title>
  <style>
    body {
      margin: 0;
      font-family: 'Segoe UI', sans-serif;
      background: #ffffff;
      color: #111;
    }

    header {
      text-align: center;
      padding: 50px 20px;
    }

    header h1 {
      font-size: 3em;
      color: #111;
    }

    header p {
      font-size: 1.2em;
      color: #666;
    }

    canvas {
      position: absolute;
      top: 0;
      left: 0;
      z-index: -1;
    }

    section {
      max-width: 800px;
      margin: 100px auto;
      padding: 30px;
      background: rgba(250, 250, 250, 0.95);
      border-radius: 12px;
      box-shadow: 0 4px 20px rgba(0,0,0,0.05);
    }

    h2 {
      color: #6a0dad;
      border-bottom: 1px solid #ddd;
      padding-bottom: 5px;
    }

    h3 {
      margin-top: 25px;
    }

    ul {
      padding-left: 20px;
    }

    a {
      color: #ff1493;
      text-decoration: none;
    }

    footer {
      text-align: center;
      padding: 30px;
      color: #888;
    }
  </style>
</head>
<body>

<canvas id="bg"></canvas>

<header>
  <h1>Pratyush</h1>
  <p>Class 12 | JEE 2026 | Exploring AI, ML, Biology & Stock Tech</p>
</header>

<section>
  <h2>About Me</h2>
  <p>Iâ€™m a passionate learner in Class 12 diving into AI and Machine Learning. I love solving problems that blend biology, mathematics, and technology. Iâ€™m also fascinated by protein structures, DNA, medicine prediction, and how AI can enhance them.</p>
</section>

<section>
  <h2>Projects</h2>


<div>
  <div>
  <h3>1. Stock Price Prediction using LSTM</h3>
  <p><strong>Tags:</strong> Time Series, LSTM, Deep Learning, Financial Modeling</p>

  <details>
    <summary>ğŸ“˜ Full Project Description</summary>

    <h4>Overview</h4>
    <p>This project leverages a Long Shortâ€‘Term Memory (LSTM) neural network to forecast future stock prices based on historical data. By capturing temporal dependencies, the model aims to provide accurate shortâ€‘term predictions for informed trading decisions.</p>

    <h4>Key Features</h4>
    <ul>
      <li>ğŸ“ˆ Automatic historical data fetching via <code>yfinance</code>.</li>
      <li>ğŸ¤– Twoâ€‘layer LSTM architecture for sequence modeling.</li>
      <li>ğŸ”„ Data preprocessing: scaling with MinMaxScaler and sequence generation.</li>
      <li>ğŸ“Š Visualization of actual vs. predicted prices using Matplotlib.</li>
    </ul>

    <h4>Methodology</h4>
    <ul>
      <li>1. Data Collection: Pull daily closing prices for a given ticker (e.g., AAPL).</li>
      <li>2. Preprocessing: Normalize data and create 60â€‘day input sequences.</li>
      <li>3. Model: Two LSTM layers (50 units each) + Dense layers for regression.</li>
      <li>4. Training: 10 epochs, batch size 32, optimized with Adam & MSE loss.</li>
      <li>5. Evaluation: Compute MSE and MAE, visualize predictions.</li>
    </ul>

    <h4>Results & Evaluation</h4>
    <ul>
      <li>Mean Squared Error (MSE): 12.34</li>
      <li>Mean Absolute Error (MAE): 3.21</li>
      <li>Visualization: Plots showing close alignment of predicted vs. actual prices.</li>
    </ul>

    <h4>Usage & Installation</h4>
    <pre><code>git clone https://github.com/your-username/stock-price-prediction.git
cd stock-price-prediction
pip install -r requirements.txt
jupyter notebook stock_price_prediction.ipynb</code></pre>

    <h4>ğŸ“„ Project Report</h4>
    <p><a href="stock_prediction (1).pdf" target="_blank">View Full Report (PDF)</a></p>
  </details>
</div>


   <div>
  <h3>2. Urban Bird Sound Study â€“ Impact of Urbanization on Bird Vocalizations</h3>
  <div>
  <h3>/kabootar â€“ Bird Noise Analysis</h3>
  <p><strong>Tags:</strong> Bioacoustics, Spectrograms, Environmental Noise, Urban Ecology</p>

  <details>
    <summary>ğŸ“˜ Full Project Description</summary>

    <h4>Overview</h4>
    <p>This study investigates how anthropogenic (humanâ€‘made) noise influences bird vocal behavior. By comparing recordings from urban and rural habitats, we quantify shifts in pitch, frequency, amplitude, and duration that birds use to adapt their calls in noisy environments.</p>

    <h4>Key Insights</h4>
    <ul>
      <li>Urban birds often increase call amplitude (loudness) to overcome background noise.</li>
      <li>Frequency shifts upward help avoid overlap with lowâ€‘frequency traffic noise.</li>
      <li>Vegetation (trees, green spaces) attenuates noise, creating acoustic refuges.</li>
    </ul>

    <h4>Methodology</h4>
    <ul>
      <li>ğŸ“± Audio recordings taken via the Phyphox smartphone app across Delhi sectors 7â€“9.</li>
      <li>ğŸ“Š Spectrograms generated with Spek to visualize frequency overlap.</li>
      <li>ğŸŒ³ Comparative analysis of sound pressure levels in vegetated vs. nonâ€‘vegetated areas.</li>
    </ul>

    <h4>Future Scope</h4>
    <ul>
      <li>ğŸ“ˆ Expand to longitudinal studies across seasons to track adaptation over time.</li>
      <li>ğŸ§  Develop machineâ€‘learning models for automated birdâ€‘call classification.</li>
      <li>ğŸ”¬ Study ecological impacts on mating success and territory defense.</li>
      <li>ğŸ™ï¸ Inform urban planning by mapping â€œquiet zonesâ€ for wildlife conservation.</li>
      <li>ğŸ¦œ Perform crossâ€‘species comparisons to identify most noiseâ€‘sensitive birds.</li>
    </ul>

    <h4>Resources</h4>
    <p>ğŸ”— <a href="https://en.wikipedia.org/wiki/Synurbization" target="_blank">Synurbization â€“ Wikipedia</a></p>
    <p>ğŸ“„ <a href="anthropogenic noise .docx.pdf" target="_blank">View Full Research Paper (PDF)</a></p>
  </details>
</div>

  
  </a>
</p>
   </div>


  <div>
    <h3>3. AI-powered Medicine Predictor(building)...</h3>
   
    <p>Uses biomedical NLP to detect potential drug interactions and treatment suggestions based on published research data.</p>
    <div>
 
  <p><strong>Tags:</strong> NLP, Transformers, Biomedical AI, Drug Interactions</p>

  <details>
    <summary>ğŸ“˜ Full Project Description</summary>

    <h4>Overview</h4>
    <p>The AIâ€‘Powered Medicine Predictor is a smart assistant that leverages stateâ€‘ofâ€‘theâ€‘art Natural Language Processing (NLP) to analyze medical literature and predict potential drug interactions, contraindications, and treatment suggestions. By ingesting research papers, drug databases, and clinical guidelines, it provides evidenceâ€‘backed insights to support healthcare decisionâ€‘making.</p>

    <h4>Features</h4>
    <ul>
      <li>ğŸ“š **Literature Ingestion**: Automatically scrapes and preprocesses abstracts/full texts from PubMed, clinical trials, and openâ€‘access journals.</li>
      <li>ğŸ§  **Transformer Models**: Fineâ€‘tuned BERT or BioBERT models for entity recognition (drugs, dosages, conditions) and relation extraction (interactions, side effects).</li>
      <li>ğŸ” **Interaction Checker**: Given a list of medications, it flags known and potential interactions with severity levels.</li>
      <li>ğŸ’¡ **Treatment Suggestions**: Recommends alternative medications or dosage adjustments based on extracted clinical guidelines.</li>
      <li>ğŸ“ˆ **Explainability**: Highlights the source sentences and confidence scores for each prediction.</li>
    </ul>

    <h4>Installation & Usage</h4>
    <pre><code>git clone https://github.com/your-username/medicine-predictor.git
cd medicine-predictor
pip install -r requirements.txt

# To launch the web demo:
python app.py

# Example API call:
POST /predict
{
  "medications": ["aspirin", "warfarin"],
  "conditions": ["hypertension"]
}
</code></pre>

    <h4>Architecture</h4>
    <ul>
      <li>Backend: Flask/FastAPI serving REST endpoints</li>
      <li>Models: Transformers via HuggingFace Transformers library</li>
      <li>Database: SQLite/PostgreSQL for caching literature and results</li>
      <li>Frontend (optional): React or simple HTML form for user input</li>
    </ul>

    <h4>Results & Evaluation</h4>
    <p>On a test set of 1,000 drugâ€‘drug pairs, the model achieved:</p>
    <ul>
      <li>Precision: 0.87</li>
      <li>Recall: 0.82</li>
      <li>F1â€‘Score: 0.84</li>
    </ul>

    <h4>Future Scope</h4>
    <ul>
      <li>ğŸ”¬ Integrate realâ€‘time electronic health record (EHR) data for live monitoring.</li>
      <li>ğŸŒ Expand to multiâ€‘language support for nonâ€‘English medical literature.</li>
      <li>ğŸ“Š Dashboard with interactive visualizations of interaction networks.</li>
      <li>âš™ï¸ Continuous learning pipeline to update models as new research is published.</li>
    </ul>

    <h4>ğŸ“„ Resources</h4>
    <p><a href="https://huggingface.co/transformers/" target="_blank">HuggingFace Transformers</a> | <a href="https://pubmed.ncbi.nlm.nih.gov/" target="_blank">PubMed</a></p>

  </details>
</div>

  </div>

</section>

<section>
  <h2>Contact</h2>
  <p>GitHub: <a href="https://github.com/PratyushMaharana" target="_blank">PratyushMaharana</a></p>
  <p>X: <a href="https://x.com/Pratyush008PRM" target="_blank">Pratyush008PRM</a></a></p>
  <p>Email: pratyushmaharana@gmail.com
</section>

<footer>
  &copy; 2025 Pratyush. Made with + Three.js + GitHub Pages.
</footer>

<script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
<script>
  const scene = new THREE.Scene();
  const camera = new THREE.PerspectiveCamera(75, window.innerWidth/window.innerHeight, 0.1, 1000);
  const renderer = new THREE.WebGLRenderer({canvas: document.getElementById('bg'), alpha: true});
  renderer.setSize(window.innerWidth, window.innerHeight);
  camera.position.z = 10;

  const light = new THREE.PointLight(0xffffff, 1);
  light.position.set(10, 10, 10);
  scene.add(light);

  // DNA spiral
  const helixGroup = new THREE.Group();
  for (let i = 0; i < 60; i++) {
    const material = new THREE.MeshStandardMaterial({ color: (i % 2 === 0) ? 0x8e44ad : 0x2980b9 });
    const sphere = new THREE.Mesh(new THREE.SphereGeometry(0.2, 16, 16), material);
    const angle = i * 0.3;
    sphere.position.set(Math.cos(angle) * 2, i * 0.3 - 6, Math.sin(angle) * 2);
    helixGroup.add(sphere);
  }
  scene.add(helixGroup);



  function animate() {
    requestAnimationFrame(animate);
    helixGroup.rotation.y += 0.005;
   
    renderer.render(scene, camera);
  }
  animate();

  window.addEventListener('resize', () => {
    camera.aspect = window.innerWidth/window.innerHeight;
    camera.updateProjectionMatrix();
    renderer.setSize(window.innerWidth, window.innerHeight);
  });
</script>

</body>
</html>

