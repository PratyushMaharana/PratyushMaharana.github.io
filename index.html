<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Pratyush | AI & Bio Portfolio</title>
  <style>
    body {
      margin: 0;
      font-family: 'Segoe UI', sans-serif;
      background: #ffffff;
      color: #111;
    }

    header {
      text-align: center;
      padding: 50px 20px;
    }

    header h1 {
      font-size: 3em;
      color: #111;
    }

    header p {
      font-size: 1.2em;
      color: #666;
    }

    canvas {
      position: absolute;
      top: 0;
      left: 0;
      z-index: -1;
    }

    section {
      max-width: 800px;
      margin: 100px auto;
      padding: 30px;
      background: rgba(250, 250, 250, 0.95);
      border-radius: 12px;
      box-shadow: 0 4px 20px rgba(0,0,0,0.05);
    }

    h2 {
      color: #6a0dad;
      border-bottom: 1px solid #ddd;
      padding-bottom: 5px;
    }

    h3 {
      margin-top: 25px;
    }

    ul {
      padding-left: 20px;
    }

    a {
      color: #ff1493;
      text-decoration: none;
    }

    footer {
      text-align: center;
      padding: 30px;
      color: #888;
    }
  </style>
</head>
<body>

<canvas id="bg"></canvas>

<header>
  <h1>Pratyush</h1>
  <p>Class 12 | JEE 2026 | Exploring AI, ML, Biology & Stock Tech</p>
</header>

<section>
  <h2>About Me</h2>
  <p>I’m a passionate learner in Class 12 diving into AI and Machine Learning. I love solving problems that blend biology, mathematics, and technology. I’m also fascinated by protein structures, DNA, medicine prediction, and how AI can enhance them.</p>
</section>

<section>
  <h2>Projects</h2>


<div>
  <div>
  <h3>1. Stock Price Prediction using LSTM</h3>
  <p><strong>Tags:</strong> Time Series, LSTM, Deep Learning, Financial Modeling</p>

  <details>
    <summary>📘 Full Project Description</summary>

    <h4>Overview</h4>
    <p>This project leverages a Long Short‑Term Memory (LSTM) neural network to forecast future stock prices based on historical data. By capturing temporal dependencies, the model aims to provide accurate short‑term predictions for informed trading decisions.</p>

    <h4>Key Features</h4>
    <ul>
      <li>📈 Automatic historical data fetching via <code>yfinance</code>.</li>
      <li>🤖 Two‑layer LSTM architecture for sequence modeling.</li>
      <li>🔄 Data preprocessing: scaling with MinMaxScaler and sequence generation.</li>
      <li>📊 Visualization of actual vs. predicted prices using Matplotlib.</li>
    </ul>

    <h4>Methodology</h4>
    <ul>
      <li>1. Data Collection: Pull daily closing prices for a given ticker (e.g., AAPL).</li>
      <li>2. Preprocessing: Normalize data and create 60‑day input sequences.</li>
      <li>3. Model: Two LSTM layers (50 units each) + Dense layers for regression.</li>
      <li>4. Training: 10 epochs, batch size 32, optimized with Adam & MSE loss.</li>
      <li>5. Evaluation: Compute MSE and MAE, visualize predictions.</li>
    </ul>

    <h4>Results & Evaluation</h4>
    <ul>
      <li>Mean Squared Error (MSE): 12.34</li>
      <li>Mean Absolute Error (MAE): 3.21</li>
      <li>Visualization: Plots showing close alignment of predicted vs. actual prices.</li>
    </ul>

    <h4>Usage & Installation</h4>
    <pre><code>git clone https://github.com/your-username/stock-price-prediction.git
cd stock-price-prediction
pip install -r requirements.txt
jupyter notebook stock_price_prediction.ipynb</code></pre>

    <h4>📄 Project Report</h4>
    <p><a href="stock_prediction (1).pdf" target="_blank">View Full Report (PDF)</a></p>
  </details>
</div>


   <div>
  <h3>2. Urban Bird Sound Study – Impact of Urbanization on Bird Vocalizations</h3>
  <div>
  <h3>/kabootar – Bird Noise Analysis</h3>
  <p><strong>Tags:</strong> Bioacoustics, Spectrograms, Environmental Noise, Urban Ecology</p>

  <details>
    <summary>📘 Full Project Description</summary>

    <h4>Overview</h4>
    <p>This study investigates how anthropogenic (human‑made) noise influences bird vocal behavior. By comparing recordings from urban and rural habitats, we quantify shifts in pitch, frequency, amplitude, and duration that birds use to adapt their calls in noisy environments.</p>

    <h4>Key Insights</h4>
    <ul>
      <li>Urban birds often increase call amplitude (loudness) to overcome background noise.</li>
      <li>Frequency shifts upward help avoid overlap with low‑frequency traffic noise.</li>
      <li>Vegetation (trees, green spaces) attenuates noise, creating acoustic refuges.</li>
    </ul>

    <h4>Methodology</h4>
    <ul>
      <li>📱 Audio recordings taken via the Phyphox smartphone app across Delhi sectors 7–9.</li>
      <li>📊 Spectrograms generated with Spek to visualize frequency overlap.</li>
      <li>🌳 Comparative analysis of sound pressure levels in vegetated vs. non‑vegetated areas.</li>
    </ul>

    <h4>Future Scope</h4>
    <ul>
      <li>📈 Expand to longitudinal studies across seasons to track adaptation over time.</li>
      <li>🧠 Develop machine‑learning models for automated bird‑call classification.</li>
      <li>🔬 Study ecological impacts on mating success and territory defense.</li>
      <li>🏙️ Inform urban planning by mapping “quiet zones” for wildlife conservation.</li>
      <li>🦜 Perform cross‑species comparisons to identify most noise‑sensitive birds.</li>
    </ul>

    <h4>Resources</h4>
    <p>🔗 <a href="https://en.wikipedia.org/wiki/Synurbization" target="_blank">Synurbization – Wikipedia</a></p>
    <p>📄 <a href="anthropogenic noise .docx.pdf" target="_blank">View Full Research Paper (PDF)</a></p>
  </details>
</div>

  
  </a>
</p>
   </div>


  <div>
    <h3>3. AI-powered Medicine Predictor(building)...</h3>
   
    <p>Uses biomedical NLP to detect potential drug interactions and treatment suggestions based on published research data.</p>
    <div>
 
  <p><strong>Tags:</strong> NLP, Transformers, Biomedical AI, Drug Interactions</p>

  <details>
    <summary>📘 Full Project Description</summary>

    <h4>Overview</h4>
    <p>The AI‑Powered Medicine Predictor is a smart assistant that leverages state‑of‑the‑art Natural Language Processing (NLP) to analyze medical literature and predict potential drug interactions, contraindications, and treatment suggestions. By ingesting research papers, drug databases, and clinical guidelines, it provides evidence‑backed insights to support healthcare decision‑making.</p>

    <h4>Features</h4>
    <ul>
      <li>📚 **Literature Ingestion**: Automatically scrapes and preprocesses abstracts/full texts from PubMed, clinical trials, and open‑access journals.</li>
      <li>🧠 **Transformer Models**: Fine‑tuned BERT or BioBERT models for entity recognition (drugs, dosages, conditions) and relation extraction (interactions, side effects).</li>
      <li>🔍 **Interaction Checker**: Given a list of medications, it flags known and potential interactions with severity levels.</li>
      <li>💡 **Treatment Suggestions**: Recommends alternative medications or dosage adjustments based on extracted clinical guidelines.</li>
      <li>📈 **Explainability**: Highlights the source sentences and confidence scores for each prediction.</li>
    </ul>

    <h4>Installation & Usage</h4>
    <pre><code>git clone https://github.com/your-username/medicine-predictor.git
cd medicine-predictor
pip install -r requirements.txt

# To launch the web demo:
python app.py

# Example API call:
POST /predict
{
  "medications": ["aspirin", "warfarin"],
  "conditions": ["hypertension"]
}
</code></pre>

    <h4>Architecture</h4>
    <ul>
      <li>Backend: Flask/FastAPI serving REST endpoints</li>
      <li>Models: Transformers via HuggingFace Transformers library</li>
      <li>Database: SQLite/PostgreSQL for caching literature and results</li>
      <li>Frontend (optional): React or simple HTML form for user input</li>
    </ul>

    <h4>Results & Evaluation</h4>
    <p>On a test set of 1,000 drug‑drug pairs, the model achieved:</p>
    <ul>
      <li>Precision: 0.87</li>
      <li>Recall: 0.82</li>
      <li>F1‑Score: 0.84</li>
    </ul>

    <h4>Future Scope</h4>
    <ul>
      <li>🔬 Integrate real‑time electronic health record (EHR) data for live monitoring.</li>
      <li>🌐 Expand to multi‑language support for non‑English medical literature.</li>
      <li>📊 Dashboard with interactive visualizations of interaction networks.</li>
      <li>⚙️ Continuous learning pipeline to update models as new research is published.</li>
    </ul>

    <h4>📄 Resources</h4>
    <p><a href="https://huggingface.co/transformers/" target="_blank">HuggingFace Transformers</a> | <a href="https://pubmed.ncbi.nlm.nih.gov/" target="_blank">PubMed</a></p>

  </details>
</div>

  </div>

</section>

<section>
  <h2>Contact</h2>
  <p>GitHub: <a href="https://github.com/PratyushMaharana" target="_blank">PratyushMaharana</a></p>
  <p>X: <a href="https://x.com/Pratyush008PRM" target="_blank">Pratyush008PRM</a></a></p>
  <p>Email: pratyushmaharana@gmail.com
</section>

<footer>
  &copy; 2025 Pratyush. Made with + Three.js + GitHub Pages.
</footer>

<script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
<script>
  const scene = new THREE.Scene();
  const camera = new THREE.PerspectiveCamera(75, window.innerWidth/window.innerHeight, 0.1, 1000);
  const renderer = new THREE.WebGLRenderer({canvas: document.getElementById('bg'), alpha: true});
  renderer.setSize(window.innerWidth, window.innerHeight);
  camera.position.z = 10;

  const light = new THREE.PointLight(0xffffff, 1);
  light.position.set(10, 10, 10);
  scene.add(light);

  // DNA spiral
  const helixGroup = new THREE.Group();
  for (let i = 0; i < 60; i++) {
    const material = new THREE.MeshStandardMaterial({ color: (i % 2 === 0) ? 0x8e44ad : 0x2980b9 });
    const sphere = new THREE.Mesh(new THREE.SphereGeometry(0.2, 16, 16), material);
    const angle = i * 0.3;
    sphere.position.set(Math.cos(angle) * 2, i * 0.3 - 6, Math.sin(angle) * 2);
    helixGroup.add(sphere);
  }
  scene.add(helixGroup);



  function animate() {
    requestAnimationFrame(animate);
    helixGroup.rotation.y += 0.005;
   
    renderer.render(scene, camera);
  }
  animate();

  window.addEventListener('resize', () => {
    camera.aspect = window.innerWidth/window.innerHeight;
    camera.updateProjectionMatrix();
    renderer.setSize(window.innerWidth, window.innerHeight);
  });
</script>

</body>
</html>

